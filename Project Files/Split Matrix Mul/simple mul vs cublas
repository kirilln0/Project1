// Utilities and system includes

#pragma comment(lib,"cublas.lib")

#include "device_launch_parameters.h"

#include <stdlib.h>
#include <math.h>
#include "cublas_v2.h"

#include <stdio.h>

#include <iostream>
#include <iomanip>
#include <cmath>
#include <chrono>

#include <assert.h>
#include "helper_string.h"  // helper for shared functions common to CUDA Samples

// CUDA runtime
#include <cuda_runtime.h>

// CUDA and CUBLAS functions
#include "helper_functions.h"
#include "helper_cuda.h"

// Input size
int const BATCH = 1; 
int const DEPTH = 3;
int const WIDTH = 2048;
int const LENGTH = 2048;
// Kernel characteristics
int const ZPADX = 0;
int const ZPADY = 0;
int const STRIDEX = 1;
int const STRIDEY = 1;
int const CONV_RECP_SIZEX = 3;
int const CONV_RECP_SIZEY = 3;
int const NUM_OF_KERNELS = 1;
// Convolution output characteristics
int const convLayerSizeX = ((WIDTH - CONV_RECP_SIZEX + 2 * ZPADX) / STRIDEX + 1);
int const convLayerSizeY = ((LENGTH - CONV_RECP_SIZEY + 2 * ZPADY) / STRIDEY + 1);
// transformation matrix characteristics
int const transformSizeY = convLayerSizeY * convLayerSizeX;
int const transformSizeX = CONV_RECP_SIZEX * CONV_RECP_SIZEY * DEPTH;
int PRINT_MATS = 0;
int CPU_COMPARE = 0;

#define NTHREADS_X 32
#define NTHREADS_Y 32
#define THREADS_PER_BLOCK NTHREADS_X * NTHREADS_Y

const float alpha = 1.0f;
const float beta = 0.0f;

#ifndef min
#define min(a,b) ((a < b) ? a : b)
#endif
#ifndef max
#define max(a,b) ((a > b) ? a : b)
#endif

typedef struct _matrixSize      // Optional Command-line multiplier for matrix sizes
{
	unsigned int uiWA, uiHA, uiWB, uiHB, uiWC, uiHC;
} sMatrixSize;

////////////////////////////////////////////////////////////////////////////////
//! Compute reference data set matrix multiply on CPU
//! C = A * B
//! @param C          reference data, computed but preallocated
//! @param A          matrix A as provided to device
//! @param B          matrix B as provided to device
//! @param hA         height of matrix A
//! @param wB         width of matrix B
////////////////////////////////////////////////////////////////////////////////
void
matrixMulCPU(float *C, const float *A, const float *B, unsigned int hA, unsigned int wA, unsigned int wB)
{
	for (unsigned int i = 0; i < hA; ++i)
		for (unsigned int j = 0; j < wB; ++j)
		{
			double sum = 0;

			for (unsigned int k = 0; k < wA; ++k)
			{
				double a = A[i * wA + k];
				double b = B[k * wB + j];
				sum += a * b;
			}

			C[i * wB + j] = (float)sum;
		}
}


__global__ void matrix_mul(float *a, float *b, float *c, int a_ncolumns, int c_nlines,
	int c_ncolumns, int nBlocks)
{
	int i, z;
	float sum = 0;

	/* How many multiplications there will be for each value in Matrix C
	 * This corresponds to the number of columns in Matrix A (or number of)
	 * lines in Matrix B
	 */
	int nMultiplications = a_ncolumns;

	/* Each iteration of the block will multiply NTHREADS_Y values. This value
	 * Can be less then NTHREADS_Y if the number of a_ncolumns is not divisible
	 * by NTHREADS_Y. This value is used to control that.
	 */
	int multiplicationsInBlock = NTHREADS_Y;

	int column = blockIdx.x * blockDim.x + threadIdx.x;
	int line = blockIdx.y * blockDim.y + threadIdx.y;

	__shared__ float s_a[NTHREADS_Y][NTHREADS_X];
	__shared__ float s_b[NTHREADS_Y][NTHREADS_X];

	/* temporary line and temporary column
	 * Each thread is responsible for loading one value in the matrix A and
	 * Matrix B. These variables are used to hold which line and column of the
	 * original Matrices they are suppose to load. I also need to check if those
	 * values that they will load actually correspond to a valid position in the
	 * original Matrix.
	 */
	int a_tLine, a_tColumn, b_tLine, b_tColumn;

	for (z = 0; z < nBlocks; z++)
	{

		// Load Matrix A
		a_tLine = (blockIdx.y * NTHREADS_Y + threadIdx.y);
		a_tColumn = (z * NTHREADS_X + threadIdx.x);
		if (a_tLine < c_nlines && a_tColumn < a_ncolumns)
		{
			s_a[threadIdx.y][threadIdx.x] = a[(a_ncolumns * a_tLine) + a_tColumn];
		}

		// Load Matrix B
		b_tLine = (z * NTHREADS_Y + threadIdx.y);
		b_tColumn = (blockIdx.x * NTHREADS_X + threadIdx.x);
		if (b_tLine < a_ncolumns && b_tColumn < c_ncolumns)
		{
			s_b[threadIdx.y][threadIdx.x] = b[(c_ncolumns * b_tLine) + b_tColumn];
		}

		__syncthreads();

		/* Checkin to see if that thread actually belongs to a valid position in
		 * the Matrix C
		 */
		if (column < c_ncolumns && line < c_nlines)
		{
			if (nMultiplications < NTHREADS_Y)
			{
				multiplicationsInBlock = nMultiplications;
			}

			for (i = 0; i < multiplicationsInBlock; i++)
			{
				sum += s_a[threadIdx.y][i] * s_b[i][threadIdx.x];
			}

			nMultiplications -= NTHREADS_Y;
		}

		__syncthreads();
	}

	/* Checkin to see if that thread actually belongs to a valid position in
	 * the Matrix C
	 */
	if (column < c_ncolumns && line < c_nlines)
	{
		c[line * c_ncolumns + column] = sum;
	}
}

void print_matrix(const float *A, int nr_rows_A, int nr_cols_A) 
{
	for (int i = 0; i < nr_rows_A; ++i) 
	{
		for (int j = 0; j < nr_cols_A; ++j) 
		{
			std::cout << A[j * nr_rows_A + i] << " ";
		}
		std::cout << std::endl;
	}
	std::cout << std::endl;
}



// Allocates a matrix with random float entries.
void randomInit(float *data, int size)
{
	for (int i = 0; i < size; ++i)
		data[i] = rand() / (float)RAND_MAX;
}

void printDiff(float *data1, float *data2, int width, int height, int iListLength, float fListTol)
{
	printf("Listing first %d Differences > %.6f...\n", iListLength, fListTol);
	int i, j, k;
	int error_count = 0;

	for (j = 0; j < height; j++)
	{
		if (error_count < iListLength)
		{
			printf("\n  Row %d:\n", j);
		}

		for (i = 0; i < width; i++)
		{
			k = j * width + i;
			float fDiff = fabs(data1[k] - data2[k]);

			if (fDiff > fListTol)
			{
				if (error_count < iListLength)
				{
					printf("    Loc(%d,%d)\tCPU=%.5f\tGPU=%.5f\tDiff=%.6f\n", i, j, data1[k], data2[k], fDiff);
				}

				error_count++;
			}
		}
	}

	printf(" \n  Total Errors = %d\n", error_count);
}

void initializeCUDA(int argc, char **argv, int &devID, int &iSizeMultiple, sMatrixSize &matrix_size)
{
	// By default, we use device 0, otherwise we override the device ID based on what is provided at the command line
	cudaError_t error;
	devID = 0;

	devID = findCudaDevice(argc, (const char **)argv);

	if (checkCmdLineFlag(argc, (const char **)argv, "sizemult"))
	{
		iSizeMultiple = getCmdLineArgumentInt(argc, (const char **)argv, "sizemult");
	}

	iSizeMultiple = min(iSizeMultiple, 10);
	iSizeMultiple = max(iSizeMultiple, 1);

	cudaDeviceProp deviceProp;

	error = cudaGetDeviceProperties(&deviceProp, devID);

	if (error != cudaSuccess)
	{
		printf("cudaGetDeviceProperties returned error code %d, line(%d)\n", error, __LINE__);
		exit(EXIT_FAILURE);
	}

	printf("GPU Device %d: \"%s\" with compute capability %d.%d\n\n", devID, deviceProp.name, deviceProp.major, deviceProp.minor);

	int block_size = 32;

	matrix_size.uiWA = transformSizeX;//3 * block_size * iSizeMultiple;
	matrix_size.uiHA = NUM_OF_KERNELS;//4 * block_size * iSizeMultiple;
	matrix_size.uiWB = transformSizeY;//2 * block_size * iSizeMultiple;
	matrix_size.uiHB = transformSizeX; //3 * block_size * iSizeMultiple;
	matrix_size.uiWC = transformSizeY;//2 * block_size * iSizeMultiple;
	matrix_size.uiHC = NUM_OF_KERNELS;//4 * block_size * iSizeMultiple;
	std::cout << matrix_size.uiWA << "," << matrix_size.uiHA << "," << matrix_size.uiWB << "," << matrix_size.uiHB << "," << matrix_size.uiWC << "," << matrix_size.uiHC << std::endl;

	printf("MatrixA(%u,%u), MatrixB(%u,%u), MatrixC(%u,%u)\n",
		matrix_size.uiHA, matrix_size.uiWA,
		matrix_size.uiHB, matrix_size.uiWB,
		matrix_size.uiHC, matrix_size.uiWC);

	if (matrix_size.uiWA != matrix_size.uiHB ||
		matrix_size.uiHA != matrix_size.uiHC ||
		matrix_size.uiWB != matrix_size.uiWC)
	{
		printf("ERROR: Matrix sizes do not match!\n");
		exit(-1);
	}
}


int matrixMultiply(int argc, char **argv, int devID, sMatrixSize &matrix_size)
{
	cudaDeviceProp deviceProp;

	checkCudaErrors(cudaGetDeviceProperties(&deviceProp, devID));

	int block_size = 32;

	// set seed for rand()
	srand(2006);

	// allocate host memory for matrices A and B
	unsigned int size_A = matrix_size.uiWA * matrix_size.uiHA;
	unsigned int mem_size_A = sizeof(float) * size_A;
	float *h_A = (float *)malloc(mem_size_A);
	unsigned int size_B = matrix_size.uiWB * matrix_size.uiHB;
	unsigned int mem_size_B = sizeof(float) * size_B;
	float *h_B = (float *)malloc(mem_size_B);

	// set seed for rand()
	srand(2006);

	// initialize host memory
	randomInit(h_A, size_A);
	randomInit(h_B, size_B);

	// allocate device memory
	float *d_A, *d_B, *d_C;
	unsigned int size_C = matrix_size.uiWC * matrix_size.uiHC;
	unsigned int mem_size_C = sizeof(float) * size_C;

	// allocate host memory for the result
	float *h_C = (float *)malloc(mem_size_C);
	float *h_CUBLAS = (float *)malloc(mem_size_C);

	checkCudaErrors(cudaMalloc((void **)&d_A, mem_size_A));
	checkCudaErrors(cudaMalloc((void **)&d_B, mem_size_B));
	checkCudaErrors(cudaMemcpy(d_A, h_A, mem_size_A, cudaMemcpyHostToDevice));
	checkCudaErrors(cudaMemcpy(d_B, h_B, mem_size_B, cudaMemcpyHostToDevice));
	checkCudaErrors(cudaMalloc((void **)&d_C, mem_size_C));

	// setup execution parameters
	dim3 threads(block_size, block_size);
	dim3 grid(matrix_size.uiWC / threads.x, matrix_size.uiHC / threads.y);

	// create and start timer
	printf("Computing result using CUBLAS...");

	// execute the kernel
	int nIter = 1;

	// CUBLAS version 2.0
	{
		const float alpha = 1.0f;
		const float beta = 0.0f;
		cublasHandle_t handle;
		cudaEvent_t start, stop;

		checkCudaErrors(cublasCreate(&handle));

		//Perform warmup operation with cublas
		checkCudaErrors(cublasSgemm(handle, CUBLAS_OP_N, CUBLAS_OP_N, matrix_size.uiWB, matrix_size.uiHA, matrix_size.uiWA, &alpha, d_B, matrix_size.uiWB, d_A, matrix_size.uiWA, &beta, d_C, matrix_size.uiWB));

		// Allocate CUDA events that we'll use for timing
		checkCudaErrors(cudaEventCreate(&start));
		checkCudaErrors(cudaEventCreate(&stop));

		// Record the start event
		checkCudaErrors(cudaEventRecord(start, NULL));

		for (int j = 0; j < nIter; j++)
		{
			//note cublas is column primary!
			//need to transpose the order
			checkCudaErrors(cublasSgemm(handle, CUBLAS_OP_N, CUBLAS_OP_N, matrix_size.uiWB, matrix_size.uiHA, matrix_size.uiWA, &alpha, d_B, matrix_size.uiWB, d_A, matrix_size.uiWA, &beta, d_C, matrix_size.uiWB));

		}

		printf("done.\n");

		// Record the stop event
		checkCudaErrors(cudaEventRecord(stop, NULL));

		// Wait for the stop event to complete
		checkCudaErrors(cudaEventSynchronize(stop));

		float msecTotal = 0.0f;
		checkCudaErrors(cudaEventElapsedTime(&msecTotal, start, stop));

		// Compute and print the performance
		float msecPerMatrixMul = msecTotal / nIter;
		double flopsPerMatrixMul = 2.0 * (double)matrix_size.uiHC * (double)matrix_size.uiWC * (double)matrix_size.uiHB;
		double gigaFlops = (flopsPerMatrixMul * 1.0e-9f) / (msecPerMatrixMul / 1000.0f);
		printf(
			"Performance= %.2f GFlop/s, Time= %.3f msec, Size= %.0f Ops\n",
			gigaFlops,
			msecPerMatrixMul,
			flopsPerMatrixMul);

		// copy result from device to host
		checkCudaErrors(cudaMemcpy(h_CUBLAS, d_C, mem_size_C, cudaMemcpyDeviceToHost));

		// Destroy the handle
		checkCudaErrors(cublasDestroy(handle));
	}

	// compute reference solution
	if (CPU_COMPARE)
	{
		printf("Computing result using host CPU...");
		float *reference = (float *)malloc(mem_size_C);
		matrixMulCPU(reference, h_A, h_B, matrix_size.uiHA, matrix_size.uiWA, matrix_size.uiWB);
		printf("done.\n");
		// check result (CUBLAS)
		bool resCUBLAS = sdkCompareL2fe(reference, h_CUBLAS, size_C, 1.0e-6f);
		if (resCUBLAS != true)
		{
			printDiff(reference, h_CUBLAS, matrix_size.uiWC, matrix_size.uiHC, 100, 1.0e-5f);
		}
		printf("Comparing CUBLAS Matrix Multiply with CPU results: %s\n", (true == resCUBLAS) ? "PASS" : "FAIL");
		free(reference);
	}







	if (PRINT_MATS)
	{
		print_matrix(h_CUBLAS, matrix_size.uiHC, matrix_size.uiWC);
		print_matrix(h_A, matrix_size.uiHA, matrix_size.uiWA);
		print_matrix(h_B, matrix_size.uiHB, matrix_size.uiWB);
	}


	// clean up memory
	free(h_A);
	free(h_B);
	free(h_C);

	checkCudaErrors(cudaFree(d_A));
	checkCudaErrors(cudaFree(d_B));
	checkCudaErrors(cudaFree(d_C));

	return EXIT_SUCCESS;    // return value = 1

}


int matrixHomeMultiply(int argc, char **argv, int devID, sMatrixSize &matrix_size)
{
	cudaDeviceProp deviceProp;

	checkCudaErrors(cudaGetDeviceProperties(&deviceProp, devID));

	int block_size = 32;

	// set seed for rand()
	srand(2006);

	// allocate host memory for matrices A and B
	unsigned int size_A = matrix_size.uiWA * matrix_size.uiHA;
	unsigned int mem_size_A = sizeof(float) * size_A;
	float *h_A = (float *)malloc(mem_size_A);
	unsigned int size_B = matrix_size.uiWB * matrix_size.uiHB;
	unsigned int mem_size_B = sizeof(float) * size_B;
	float *h_B = (float *)malloc(mem_size_B);

	// set seed for rand()
	srand(2006);

	// initialize host memory
	randomInit(h_A, size_A);
	randomInit(h_B, size_B);

	// allocate device memory
	float *d_A, *d_B, *d_C;
	unsigned int size_C = matrix_size.uiWC * matrix_size.uiHC;
	unsigned int mem_size_C = sizeof(float) * size_C;

	// allocate host memory for the result
	float *h_C = (float *)malloc(mem_size_C);
	float *h_CUBLAS = (float *)malloc(mem_size_C);

	checkCudaErrors(cudaMalloc((void **)&d_A, mem_size_A));
	checkCudaErrors(cudaMalloc((void **)&d_B, mem_size_B));
	checkCudaErrors(cudaMemcpy(d_A, h_A, mem_size_A, cudaMemcpyHostToDevice));
	checkCudaErrors(cudaMemcpy(d_B, h_B, mem_size_B, cudaMemcpyHostToDevice));
	checkCudaErrors(cudaMalloc((void **)&d_C, mem_size_C));

	// setup execution parameters
	dim3 threads(block_size, block_size);
	dim3 grid(matrix_size.uiWC / threads.x, matrix_size.uiHC / threads.y);

	// create and start timer
	printf("Computing result using CUBLAS...");

	// execute the kernel
	int nIter = 1;

	{
		cudaEvent_t start, stop;

		dim3 tbloco = dim3(
			(int)std::ceil((double)matrix_size.uiWC / NTHREADS_X),
			(int)std::ceil((double)matrix_size.uiHC / NTHREADS_Y),
			1
		);

		dim3 tthreads = dim3(
			NTHREADS_X,
			NTHREADS_Y,
			1
		);

		// Allocate CUDA events that we'll use for timing
		checkCudaErrors(cudaEventCreate(&start));
		checkCudaErrors(cudaEventCreate(&stop));

		// Record the start event
		checkCudaErrors(cudaEventRecord(start, NULL));

		for (int j = 0; j < nIter; j++)
		{
			matrix_mul << <tbloco, tthreads >> > (d_A, d_B, d_C, matrix_size.uiWA, matrix_size.uiHC,
				matrix_size.uiWC, (int)std::ceil((double)matrix_size.uiWA / NTHREADS_X));
		}

		printf("done.\n");

		// Record the stop event
		checkCudaErrors(cudaEventRecord(stop, NULL));

		// Wait for the stop event to complete
		checkCudaErrors(cudaEventSynchronize(stop));

		float msecTotal = 0.0f;
		checkCudaErrors(cudaEventElapsedTime(&msecTotal, start, stop));

		// Compute and print the performance
		float msecPerMatrixMul = msecTotal / nIter;
		double flopsPerMatrixMul = 2.0 * (double)matrix_size.uiHC * (double)matrix_size.uiWC * (double)matrix_size.uiHB;
		double gigaFlops = (flopsPerMatrixMul * 1.0e-9f) / (msecPerMatrixMul / 1000.0f);
		printf(
			"Performance= %.2f GFlop/s, Time= %.3f msec, Size= %.0f Ops\n",
			gigaFlops,
			msecPerMatrixMul,
			flopsPerMatrixMul);

		// copy result from device to host
		checkCudaErrors(cudaMemcpy(h_CUBLAS, d_C, mem_size_C, cudaMemcpyDeviceToHost));

	}

	// compute reference solution
	if (CPU_COMPARE)
	{
		printf("Computing result using host CPU...");
		float *reference = (float *)malloc(mem_size_C);
		matrixMulCPU(reference, h_A, h_B, matrix_size.uiHA, matrix_size.uiWA, matrix_size.uiWB);
		printf("done.\n");
		// check result (CUBLAS)
		bool resCUBLAS = sdkCompareL2fe(reference, h_CUBLAS, size_C, 1.0e-6f);
		if (resCUBLAS != true)
		{
			printDiff(reference, h_CUBLAS, matrix_size.uiWC, matrix_size.uiHC, 100, 1.0e-5f);
		}
		printf("Comparing CUBLAS Matrix Multiply with CPU results: %s\n", (true == resCUBLAS) ? "PASS" : "FAIL");
		free(reference);
	}







	if (PRINT_MATS)
	{
		print_matrix(h_CUBLAS, matrix_size.uiHC, matrix_size.uiWC);
		print_matrix(h_A, matrix_size.uiHA, matrix_size.uiWA);
		print_matrix(h_B, matrix_size.uiHB, matrix_size.uiWB);
	}


	// clean up memory
	free(h_A);
	free(h_B);
	free(h_C);

	checkCudaErrors(cudaFree(d_A));
	checkCudaErrors(cudaFree(d_B));
	checkCudaErrors(cudaFree(d_C));

	return EXIT_SUCCESS;    // return value = 1


}

////////////////////////////////////////////////////////////////////////////////
// Program main
////////////////////////////////////////////////////////////////////////////////
int main(int argc, char **argv)
{
	printf("[Matrix Multiply CUBLAS] - Starting...\n");

	int devID = 0, sizeMult = 5;
	double split_percentage = 0.20, prediction_success = 0.25;
	sMatrixSize matrix_size;

	initializeCUDA(argc, argv, devID, sizeMult, matrix_size);
	std::cout << "\nCublas matrix Multiply:" << std::endl;
	int matrix_result1 = matrixMultiply(argc, argv, devID, matrix_size);
	
	std::cout << "\nHomemade matrix Multiply:" << std::endl;
	matrixHomeMultiply(argc, argv, devID, matrix_size);

	
	return matrix_result1;
}
